Techniques from linear algebra are also used in analytic geometry, engineering, physics, natural sciences, computer science, computer animation, advanced facial recognition algorithms and the social sciences (particularly in economics).
Then, ''z'' can be substituted into ''L''2, which can then be solved to obtain
In quantum mechanics, the physical state of a particle is represented by a vector, and observables (such as momentum, energy, and angular momentum) are represented by linear operators on the underlying vector space.
Thus, the columns of the matrix A are the image of the basis vectors of ''E'' in '''R'''.
In France during the 1960s, educators attempted to teach linear algebra through affine dimensional vector spaces in the first year of secondary school.
For instance, not all modules have a basis (those that do are called free modules), the rank of a free module is not necessarily unique, not every linearly independent subset of a module can be extended to form a basis, and not every subset of a module that spans the space contains a basis.
Two matrices that encode the same linear transformation in different bases are called similar.
Let '''i'''=(1,0) and '''j''' =(0,1) be the natural basis vectors on ''E'', so that '''x'''=x'''i'''+y'''j'''.
The set of points with coordinates that satisfy a linear equation forms a hyperplane in an ''n''-dimensional space.
Commutativity of addition
For there to be nontrivial solutions to that equation, det(''T'' − λ I) = 0.
:: with equality only for ''v'' = 0.
The condition that ''v''1, ''v''2, ..., ''vn'' span ''V'' guarantees that each vector ''v'' can be assigned coordinates, whereas the linear independence of ''v''1, ''v''2, ..., ''vn'' assures that these coordinates are unique (i.e.
This transformation has the important property that if A'''y'''=d, then
Furthermore, if ''V'' and ''W'' are an ''n''-dimensional and ''m''-dimensional vector space over '''F''', and a basis of ''V'' and a basis of ''W'' have been fixed, then any linear transformation ''T'': ''V'' → ''W'' may be encoded by an ''m'' × ''n'' matrix ''A'' with entries in the field '''F''', called the matrix of ''T'' with respect to these bases.
An orthonormal basis is a basis where all basis vectors have length 1 and are orthogonal to each other.
Not all matrices are diagonalizable (even over an algebraically closed field).
Notice that a linear functional operates on known values for '''x'''=(x, y) to compute a value ''c'' in '''R''', while the inverse image seeks the values for '''x'''=(x, y) that yield a specific value ''c''.
For example, 2 × 2 real matrices denote standard planar mappings that preserve the origin.
The second operation, ''scalar multiplication'', takes any scalar ''a'' and any vector ''v'' and outputs a new .
If the determinant is zero, then the nullspace is nontrivial.
The first operation, ''vector addition'', takes any two vectors ''v'' and ''w'' and outputs a third vector .
Later, Gauss further developed the theory of solving linear systems by using Gaussian elimination, which was initially listed as an advancement in geodesy.
''x'' is then eliminated from ''L''3 by adding ''L''1 to ''L''3.
Below are just some examples of applications of linear algebra.
Elements of ''V'' are called ''vectors'' and elements of ''F'' are called ''scalars''.
While studying compositions of linear transformations, Arthur Cayley was led to define matrix multiplication and inverses.
If a basis of ''V'' has finite number of elements, ''V'' is called a finite-dimensional vector space.
Note that in '''R''', it is symmetric.
The vector '''h''' satisfies the homogeneous equation,
This line of inquiry naturally leads to the idea of the dual space, the vector space ''V''∗ consisting of linear maps  where ''F'' is the field of scalars.
where A'''v'''=d and A'''w'''=e are the images of the basis vectors  '''v''' and '''w'''.
This means a vector '''x''' has coordinates (α,β), such that '''x'''=α'''v'''+β'''w'''.
Let '''x'''=(x, y) be an arbitrary vector in ''E'' and consider the linear function λ: ''E''→'''R''', given by
While determinants could be defined in a basis-free manner, they are usually introduced via a specific representation of the mapping; the value of the determinant does not depend on the specific basis.
If, in addition to vector addition and scalar multiplication, there is a bilinear vector product , the vector space is called an algebra; for instance, associative algebras are algebras with an associate vector product (like the algebra of square matrices, or the algebra of polynomials).
The conditions under which a set of ''n'' hyperplanes intersect in a single point is an important focus of study in linear algebra.
There exists an element 0 ∈ ''V'', called the ''zero vector'', such that ''v'' + 0 = ''v'' for all ''v'' ∈ ''V''.
The last part, back-substitution, consists of solving for the known in reverse order.
Following work by the School Mathematics Study Group, U.S. high schools asked 12th grade students to do "matrix algebra, formerly reserved for college" in the 1960s.
The dimension of a vector space is well-defined by the dimension theorem for vector spaces.
Because an isomorphism preserves linear structure, two isomorphic vector spaces are "essentially the same" from the linear algebra point of view.
Given any finite-dimensional vector space, an orthonormal basis could be found by the Gram–Schmidt procedure.
Quantum mechanics is highly inspired by notions in linear algebra.
The first four axioms are those of ''V'' being an abelian group under vector addition.
This major technique distinguishes linear algebra from theories of other algebraic structures, which usually cannot be parameterized so concretely.
One strategy for a general n-dimensional transformation ''T'' is to find "characteristic lines" that are invariant sets under ''T''.
There is an important distinction between the coordinate ''n''-space '''R'''''n'' and a general finite-dimensional vector space ''V''.
A linear combination of any system of vectors with all zero coefficients is the zero vector of ''V''.
Therefore, we are mostly interested in a linearly independent set of vectors that spans a vector space ''V'', which we call a basis of ''V''.
This was met with a backlash in the 1980s that removed linear algebra from the curriculum.
If ''T'' satisfies ''TT*'' = ''T*T'', we call ''T'' normal.
One often restricts consideration to finite-dimensional vector spaces.
Linear transformations have geometric significance.
It is now possible to see that
Combined with calculus, linear algebra facilitates the solution of linear systems of differential equations.
This is written in matrix form as
There are several related topics in the field of Computer Programming that utilizes much of the techniques and theorems Linear Algebra encompasses and refers to.
Identity element of addition
Another way to approach linear algebra is to consider linear functions on the two dimensional real plane ''E''='''R'''2.
The set of points in the plane ''E'' that map to the same image in '''R''' under the linear functional λ define a line in ''E''.
The development of computers led to increased research in efficient algorithms for Gaussian elimination and matrix decompositions, and linear algebra became an essential tool for modelling and simulations.
The eigenvalues of ''H'' represents the possible energies that can be observed.
Because linear algebra is such a well-developed theory, nonlinear mathematical models are sometimes approximated by linear models.
If ''V'' is finite-dimensional and ''U'' is a subspace of ''V'', then dim ''U'' ≤ dim ''V''.
This result is a system of linear equations in triangular form, and so the first part of the algorithm is complete.
The line can be considered to be the set of points '''h''' in the kernel translated by the vector '''p'''.
Formally, an ''inner product'' is a map
Functional analysis studies the infinite-dimensional version of the theory of vector spaces.
1''v'' = ''v'', where 1 denotes the multiplicative identity in '''F'''.
If ''U''1 and ''U''2 are subspaces of ''V'', then
The origin of many of these ideas is discussed in the articles on determinants and Gaussian elimination.
''a''(''u'' + ''v'') = ''au'' + ''av''
Determinants have other applications, including a systematic way of seeing if a set of vectors is linearly independent (we write the vectors as the columns of a matrix, and if the determinant of that matrix is zero, the vectors are linearly dependent).
It turns out that a mapping has an inverse if and only if the determinant has an inverse (every non-zero real or complex number has an inverse).
Crucially, Cayley used a single letter to denote a matrix, thus treating a matrix as an aggregate object.
Given a particle in some state φ, we can expand φ into a linear combination of eigenstates of ''H''.
Compatibility of scalar multiplication with field multiplication
In particular, the quantity
Representation theory studies the actions of algebraic objects on vector spaces by representing these objects as matrices.
The use of matrices in quantum mechanics, special relativity, and statistics helped spread the subject of linear algebra beyond pure mathematics.
We can define the length of a vector ''v'' in ''V'' by
A fundamental theorem of linear algebra states that all vector spaces of the same dimension are isomorphic, giving an easy way of characterizing isomorphism.
It would be particularly nice if given a transformation ''T'' taking a vector space ''V'' into itself we can find a basis for ''V'' consisting of eigenvectors.
For every ''v'' ∈ V, there exists an element −''v'' ∈ ''V'', called the ''additive inverse'' of ''v'', such that ''v'' + (−''v'') = 0
If this is the only way to express the zero vector as a linear combination of ''v''1, ''v''2, ..., ''vk'' then these vectors are linearly independent.
The concept of eigenvalues and eigenvectors is especially important.
Because operations like matrix multiplication, matrix inversion, and determinant calculation are simple on diagonal matrices, computations involving matrices are much simpler if we can bring the matrix to a diagonal form.
The study of matrix algebra first emerged in England in the mid-1800s.
Linear algebra is central to both pure and applied mathematics.
Another way to say this is that the columns of the matrix must be linearly dependent.
For convenience the free parameter x has been relabeled t.
Associativity of addition
Attention to low-dimensional examples gives an indication of the variety of their types.
Two vectors are orthogonal if .
In 1844 Hermann Grassmann published his “Theory of Extension” which included foundational new topics of what is today called linear algebra.
For this case, where the image space is a real number the map is called a linear functional.
Distributivity of scalar multiplication with respect to field addition
''H'' is also known as the Hamiltonian operator.
Another important way of forming a subspace is to take a linear combination of a set of vectors ''v''1, ''v''2, ..., ''vk'':
The system is solved.
For instance, given a transform ''T'', we can define its Hermitian conjugate ''T*'' as the linear transform satisfying
The study of linear algebra first emerged from the study of determinants, which were used to solve systems of linear equations.
We can, in general, write any system of linear equations as a matrix equation:
To find an eigenvector or an eigenvalue, we note that
The vector '''p''' defines the intersection of the line with the y-axis, known as the y-intercept.
Identity element of scalar multiplication
A vector space over a field ''F'' is a set ''V'' together with two binary operations.
Consider the linear functional a little more carefully.
Notice that if '''h''' is a solution to this homogeneous equation, then ''t'' '''h''' is also a solution.
The set of all linear combinations of vectors ''v''1, ''v''2, ..., ''vk'' is called their span, which forms a subspace.
that satisfies the following three axioms for all vectors ''u'', ''v'', ''w'' in ''V'' and all scalars ''a'' in ''F'':
Algebraic geometry considers the solutions of systems of polynomial equations.
Next, ''z'' and ''y'' can be substituted into ''L''1, which can be solved to obtain
The solution set of this equation is given by .
It turns out that if we accept the axiom of choice, every vector space has a basis; nevertheless, this basis may be unnatural, and indeed, may not even be constructible.
Distributivity of scalar multiplication with respect to vector addition
Here '''R''' denotes the set of real numbers.
These equations can be assembled into the single matrix equation,
Then, we compute the solutions of ''Ax'' = 0; that is, we find the null space ''N'' of ''A''.
In the example, ''x'' is eliminated from ''L''2 by adding (3/2)''L''1 to ''L''2.
This is true for any pair of vectors used to define coordinates in ''E''.
Suppose we select a non-orthogonal non-unit vector basis '''v''' and '''w''' to define coordinates of vectors in ''E''.
In the list below, let ''u'', ''v'' and ''w'' be arbitrary vectors in ''V'', and ''a'' and ''b'' scalars in ''F''.
The functionals σ and τ compute the components of '''x''' along the basis vectors '''v''' and '''w''', respectively, that is,
Determinants were used by Leibniz in 1693, and subsequently, Gabriel Cramer devised Cramer's Rule for solving linear systems in 1750.
Vector spaces may be diverse in nature, for example, containing functions, polynomials or matrices.
This leads to the question of how to determine the coordinates of a vector '''x''' relative to a general basis '''v''' and '''w''' in ''E''.
For instance, to find the coefficient ''ak'', we take the inner product with ''hk'':
The concepts of linear independence, span, basis, and dimension (which is called rank in module theory) still make sense.
Given a set of vectors that span a space, if any vector ''w'' is a linear combination of other vectors (and so the set is not linearly independent), then the span would remain the same if we remove ''w'' from the set.
Multilinear maps  can be described via tensor products of elements of ''V''∗.
Assume that we know the coordinates of the vectors, '''x''', '''v''' and '''w'''  in the natural basis '''i'''=(1,0) and '''j''' =(0,1).
While '''R'''''n'' has a standard basis {''e''1, ''e''2, ..., ''en''}, a vector space ''V'' typically does not come equipped with such a basis and many different bases exist (although they all consist of the same number of elements equal to the dimension of ''V'').
Our goal is two find the real numbers α, β, so that '''x'''=α'''v'''+β'''w''', that is
In 1993, the U.S.-based Linear Algebra Curriculum Study Group recommended that undergraduate linear algebra courses be given an application-based "matrix orientation" as opposed to a theoretical orientation.
For a solution to exist in the plane ''E'', the coefficient matrix ''C'' must have rank 2, which means its determinant must be zero.
Then, using back-substitution, each unknown can be solved for.
For instance, there exists a basis for the real numbers, considered as a vector space over the rationals, but no explicit basis has been constructed.
Such equations are naturally represented using the formalism of matrices and vectors.
This will put the system into triangular form.
The set of points of a linear functional that map to zero define the ''kernel'' of the linear functional.
The determinant is a polynomial, and so the eigenvalues are not guaranteed to exist if the field is '''R'''.
This line will minimize the sum of the squares of the residuals.
and by orthonormality, ; that is,
It can thus be seen that
Now ''y'' is eliminated from ''L''3 by adding −4''L''2 to ''L''3:
When a bijective linear mapping exists between two vector spaces (that is, every vector from the second space is associated with exactly one in the first), we say that the two spaces are isomorphic.
The solution of this system is characterized as follows: first, we find a particular solution ''x''0 of this equation using Gaussian elimination.
Solve for y and obtain the inverse image as the set of points,
Since linear algebra is a successful theory, its methods have been developed and generalized in other parts of mathematics.
One essential question in linear algebra is whether a mapping is an isomorphism or not, and this question can be answered by checking if the determinant is nonzero.
where I is the identity matrix.
(''a'' + ''b'')''v'' = ''av'' + ''bv''
* Linearity in the first argument:
These coordinate functionals have the properties,
Any set of vectors that spans ''V'' contains a basis, and any linearly independent set of vectors in ''V'' can be extended to a basis.
The scalar λ such that ''Tv'' = λ''v'' is called a characteristic value or '''eigenvalue''' of ''T''.
In 1882, Hüseyin Tevfik Pasha wrote the book titled "Linear Algebra".
Energy is represented as the operator , where ''V'' is the potential energy.
In module theory, one replaces the field of scalars by a ring.
He also realized the connection between matrices and determinants, and wrote "There would be many things to say about this theory of matrices which should, it seems to me, precede the theory of determinants".
Any two bases of a vector space ''V'' have the same cardinality, which is called the dimension of ''V''.
Matrix theory replaces the study of linear transformations, which were defined axiomatically, by the study of matrices, which are concrete objects.
which can be written in matrix form as
This inverse image is the set of  the points '''x'''=(x, y) that solve the equation,
Because of the ubiquity of vector spaces, linear algebra is used in many fields of mathematics, natural sciences, computer science, and social science.
and so we can call this quantity the cosine of the angle between the two vectors.
The inner product is an example of a bilinear form, and it gives the vector space a geometric structure by allowing for the definition of length and angles.
In multilinear algebra, one considers multivariable linear transformations, that is, mappings that are linear in each of a number of different variables.
For instance, abstract algebra arises by relaxing the axioms of a vector space, leading to a number of generalizations.
Orthonormal bases are particularly nice to deal with, since if ''v'' = ''a''1 ''v''1 + ... + ''an vn'', then .
The least squares method is used to determine the best fit line for a set of data.
Such a transformation is called a diagonalizable matrix since in the eigenbasis, the transformation is represented by a diagonal matrix.
The functions ''gn''(''x'') = sin(''nx'')  for ''n'' > 0 and ''hn''(''x'') = cos(''nx'') for ''n'' ≥ 0 are an orthonormal basis for the space of Fourier-expandable functions.
Under this identification, addition and scalar multiplication of vectors in ''V'' correspond to addition and scalar multiplication of their coordinate vectors in '''F'''''n''.
* Conjugate symmetry:
The operations of addition and multiplication in a vector space must satisfy the following axioms.
Functional analysis mixes the methods of linear algebra with those of mathematical analysis and studies various function spaces, such as L''p'' spaces.
If ''v'' is a non-zero vector such that ''Tv'' is a scalar multiple of ''v'', then the line through 0 and ''v'' is an invariant set under ''T'' and ''v'' is called a characteristic vector or '''eigenvector'''.
In this way, once a basis of a vector space ''V'' over '''F''' has been chosen, ''V'' may be identified with the coordinate ''n''-space '''F'''''n''.
It turns out that normal matrices are precisely the matrices that have an orthonormal system of eigenvectors that span ''V''.
The Gaussian-elimination algorithm is as follows: eliminate ''x'' from all equations below ''L''1, and then eliminate ''y'' from all equations below ''L''2.
Linear algebra is concerned with properties common to all vector spaces.
Thus, a set of linearly dependent vectors is redundant in the sense that there will be a linearly independent subset which will span the same subspace.
''u'' + (''v'' + ''w'') = (''u'' + ''v'') + ''w''
The component of ''H'' in each eigenstate determines the probability of measuring the corresponding eigenvalue, and the measurement forces the particle to assume that eigenstate (wave function collapse).
'''Signification'''
Again, in analogue with theories of other algebraic objects, linear algebra is interested in subsets of vector spaces that are themselves vector spaces; these subsets are called linear subspaces.
Then, we have the linear functional
* Positive-definiteness:
''a''(''bv'') = (''ab'')''v''
there is only one linear combination of the basis vectors that is equal to ''v'').
Linear algebra first appeared in graduate textbooks in the 1940s and in undergraduate textbooks in the 1950s.
Such an investigation is initially motivated by a system of linear equations containing several unknowns.
Clearly, this equation has the solution '''x''' = (0,0,0), which is not a point on the ''z'' = 1 plane ''E''.
The main structures of linear algebra are vector spaces.
Linear algebra took its modern form in the first half of the twentieth century, when many ideas and methods of previous centuries were generalized as abstract algebra.
To solve this equation for α, β, we compute the linear coordinate functionals σ and τ for the basis '''v''', '''w''', which are given by,
One major application of the matrix theory is calculation of determinants, a central concept in linear algebra.

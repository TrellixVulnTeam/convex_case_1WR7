The brain itself can be more sensitive than it appears to be through behavioral responses.
The research and application of speech perception must deal with several problems which result from what has been termed the lack of invariance.
In an artificial continuum between a voiceless and a voiced bilabial plosive, each new step differs from the preceding one in the amount of VOT.
Because speakers have vocal tracts of different sizes (due to sex and age especially) the resonant frequencies (formants), which are important for recognition of speech sounds, will vary in their absolute values across individuals (see Figure 3 for an illustration of this).
When describing units of perception, Liberman later abandoned articulatory movements and proceeded to the neural commands to the articulators and even later to intended articulatory gestures, thus "the neural representation of the utterance that determines the speaker's production is the distal object the listener perceives".
Thus, when perceiving a speech signal our decision about what we actually hear is based on the relative goodness of the match between the stimulus information and values of particular prototypes.
It has also been discovered that even though infants' ability to distinguish between the different phonetic properties of various languages begins to decline around the age of nine months, it is possible to reverse this process by exposing them to a new language in a sufficient way.
VOT is a primary cue signaling the difference between voiced and voiceless plosives, such as "b" and "p".
The first sound is a pre-voiced , i.e.
subjects are presented with stimuli and asked to make conscious decisions about them.
Listeners had a tendency to judge the ambiguous words (when the first segment was at the boundary between categories) according to the meaning of the whole sentence.
Not all of them give satisfactory explanations of all problems, however the research they inspired has yielded a lot of useful data.
Similarly, when recognizing a talker, all the memory traces of utterances produced by that talker are activated and the talker's identity is determined.
Although listeners perceive speech as a stream of discrete units (phonemes, syllables, and words), this linearity is difficult to see in the physical speech signal (see Figure 2 for an example).
A classic example of this situation is the observation that Japanese learners of English will have problems with identifying or distinguishing English liquid consonants  and  (see Japanese speakers learning r and l).
Another basic experiment compares recognition of naturally spoken words presented in a sentence (or at least a phrase) and the same words presented in isolation.
However, utilizing technologies such as fMRI machines, research has shown that two regions of the brain traditionally considered exclusively to process speech, Broca's and Wernicke's areas, also become active during musical activities such as listening to a sequence of musical chords.
In tests of the ability to discriminate between two sounds with varying VOT values but having a constant VOT distance from each other (20 ms for instance), listeners are likely to perform at chance level if both sounds fall within the same category and at nearly 100% level if each sound falls in a different category (see the blue discrimination curve in Figure 4).
In a research study by Patricia K. Kuhl, Feng-Ming Tsao, and Huei-Mei Liu, it was discovered that if infants are spoken to and interacted with by a native speaker of Mandarin Chinese, they can actually be conditioned to retain their ability to distinguish different speech sounds within Mandarin that are very different from speech sounds found within the English language.
Infants learn to contrast different vowel phonemes of their native language by approximately 6 months of age.
This speech information can then be used for higher-level language processes, such as word recognition.
The acoustic properties of the landmarks constitute the basis for establishing the distinctive features.
Based on these results, they proposed the notion of categorical perception as a mechanism by which humans can identify speech sounds.
Within each prototype various features may combine.
However, this correspondence or mapping has proven extremely difficult to find, even after some forty-five years of research on the problem.''
Naturally, this creates difficulties when a foreign language is encountered.
For example, one of the most studied cues in speech is voice onset time or VOT.
Expressive aphasia causes moderate difficulties for language understanding.
As for other features, the difficulties vary.
This is known as the phonemic restoration effect.
Despite this the evidence and counter-evidence for the Speech Mode Hypothesis is still unclear and needs further research.
The quality of the first phoneme changed along a continuum.
The final decision is based on multiple features or sources of information, even visual information (this explains the McGurk effect).
The speech sound signal contains a number of acoustic cues that are used in speech perception.
talker-identity) is encoded/decoded along with linguistically relevant information.
Neurophysiological methods were introduced into speech perception research for several reasons:
Also it seems that modularity is learned in perceptual systems.
Having disputed the linearity of the speech signal, the problem of segmentation arises: one encounters serious difficulties trying to delimit a stretch of speech signal as belonging to a single perceptual unit.
Gradually, as they are exposed to their native language, their perception becomes language-specific, i.e.
It has been suggested that auditory learning begins already in the pre-natal period.
Best (1995) proposed a Perceptual Assimilation Model which describes possible cross-language category assimilation patterns and predicts their consequences.
As was suggested above, reliable constant relations between a phoneme of a language and its acoustic manifestation in speech are difficult to find.
Then after they completed the first part of the experiment, the experimenters taught the aphasic patients to speech read, which is the ability to read lips.
Recently a study was performed to test if surgery helps the patients discover their symptoms post surgery than pre-surgery.
The patients also did improve their place of articulation and their manner of articulation.
Whether or not normalization actually takes place and what is its exact nature is a matter of theoretical controversy (see theories below).
The native consonantal contrasts are acquired by 11 or 12 months of age.
It is not easy to identify what acoustic cues listeners are sensitive to when perceiving a particular speech sound:
The sucking-rate and the head-turn method are some of the more traditional, behavioral methods for studying speech perception.
For cochlear implant users, it is more difficult to understand unknown speakers and sounds.
they learn how to ignore the differences within phonemic categories of the language (differences that may well be contrastive in other languages â€“ for example, English distinguishes two voicing categories of plosives, whereas Thai has three categories; infants must learn which differences are distinctive in their native language uses, and which are not).
First, the baby's normal sucking rate is established.
Three important experimental paradigms have evolved in the search to find evidence for the speech mode hypothesis.
Without the necessity of taking an active part in the test, even infants can be tested; this feature is crucial in research into acquisition processes.
Neurophysiological methods rely on utilizing information stemming from more direct and not necessarily conscious (pre-attentative) processes.
Expressive aphasic patients suffer from more regular rule governed principles in forming sentences, which is closely related to Alzheimer patients.
This is often thought of in terms of abstract representations of phonemes.
These representations can then be combined for use in word recognition and other language processes.
Research shows that infants at the age of 7.5 months cannot recognize information presented by speakers of different genders; however by the age of 10.5 months, they can detect the similarities.
So it will keep the brain stimulated even though the disease tries to disable it.
Then, increasing the VOT, it reaches zero, i.e.
it has a negative VOT.
Similar perceptual adjustment is attested for other acoustic cues as well.
The effects could be difficulty in walking, communicating, or functioning.
There are several reasons for this:
This shows that surgery does improve a patients speech perception, even though it might not cure their disease.
In both children with cochlear implants and normal hearing, vowels and voice onset time becomes prevalent in development before the ability to discriminate the place of articulation.
A speech sound is influenced by the ones that precede and the ones that follow.
For speech perception, the theory asserts that the objects of perception are actual vocal tract movements, or gestures, and not abstract phonemes or (as in the Motor Theory) events that are causally antecedent to these movements, i.e.
This can take the form of an identification test, a discrimination test, similarity rating, etc.
One of the techniques used to examine how infants perceive speech, besides the head-turn procedure mentioned above, is measuring their sucking rate.
Since these gestures are limited by the capacities of humans' articulators and listeners are sensitive to their auditory correlates, the lack of invariance simply does not exist in this model.
The cues differentiate speech sounds belonging to different phonetic categories.
For example,  in English is fronted when surrounded by coronal consonants.
and they are certainly affected by changes in speaking tempo.
A group of psychologists conducted a study to test the McGurk effect with Aphasia patients and speech reading.
The direct realist theory of speech perception (mostly associated with Carol Fowler) is a part of the more general theory of direct realism, which postulates that perception allows us to have direct awareness of the world because it involves direct recovery of the distal source of the event that is perceived.
The conclusion to make from both the identification and the discrimination test is that listeners will have different sensitivity to the same relative increase in VOT depending on whether or not the boundary between categories was crossed.
One of the basic problems in the study of speech is how to deal with the noise in the speech signal.
The theory has been criticized in terms of not being able to "provide an account of just how acoustic signals are translated into intended gestures" by listeners.
This process has been called vocal tract normalization (see Figure 3 for an example).
A lot of what has been said about SP is a matter of theory.
The perceptual abilities of children that received an implant after the age of two are significantly better than of those who were implanted in adulthood.
It may be the case that it is not necessary and maybe even not possible for a listener to recognize phonemes before recognizing higher units, like words for example.
According to this view, listeners are inspecting the incoming signal for the so-called acoustic landmarks which are particular events in the spectrum carrying information about gestures which produced them.
However, features are not just binary (true or false), there is a fuzzy value corresponding to how likely it is that a sound belongs to a particular speech category.
The process of speech perception is not necessarily uni-directional.
To provide a theoretical account of the categorical perception data, Liberman and colleagues worked out the motor theory of speech perception, where "the complicated articulatory encoding was assumed to be decoded in the perception of speech by the same processes that are involved in production" (this is referred to as analysis-by-synthesis).
A number of factors have been shown to influence perceptual performance, specifically: duration of deafness prior to implantation, age of onset of deafness, age at implantation (such age effects may be related to the Critical period hypothesis) and the duration of using an implant.
For instance instead of saying the red ball bounced, both of these patients would say bounced ball the red.
This all means that aphasic patients might benefit from learning how to speech read (lip reading).
They usually cannot fully distinguish place of articulation and voicing.
'''Expressive aphasia''': Patients who suffer from this condition typically have lesions on their left inferior frontal cortex.
Despite the great variety of different speakers and different conditions, listeners perceive vowels and consonants as constant categories.
Speech Mode Hypothesis is the idea that the perception of speech requires the use of specialized mental processing.
This can be illustrated by the fact that the acoustic properties of the phoneme  will depend on the production of the following vowel (because of coarticulation).
Flege (1995) formulated a Speech Learning Model which combines several hypotheses about second-language (L2) speech acquisition and which predicts, in simple words, that an L2 sound that is not too similar to a native-language (L1) sound will be easier to acquire than an L2 sound that is relatively similar to an L1 sound (because it will be perceived as more obviously "different" by the learner).
Listeners were asked to identify which sound they heard and to discriminate between two different sounds.
The study of speech perception is closely linked to the fields of phonology and phonetics in linguistics and cognitive psychology and perception in psychology.
This is closely related to Parkinson's disease because both of the diseases have trouble in distinguishing irregular verbs.
'''Receptive aphasia''': The patients suffer from lesions or damage located in the left temproparietal lobe.
As infants learn how to sort incoming speech sounds into categories, ignoring irrelevant differences and reinforcing the contrastive ones, their perception becomes categorical.
In recent years, there has been a model developed to create a sense of how speech perception works; this model is known as the Dual Stream Model.
The process of perceiving speech begins at the level of the sound signal and the process of audition.
If the baby perceives the newly introduced stimulus as different from the background stimulus the sucking rate will show an increase.
This is shown by the difficulty that computer speech recognition systems have with recognizing human speech.
Other studies, such as one performed by Marques et al.
Other cues differentiate sounds that are produced at different places of articulation or manners of articulation.
However, there are two significant obstacles:
Dialect and foreign accent can also cause variation, as can the social characteristics of the speaker and listener.
Categorical perception is involved in processes of perceptual differentiation.
Gradually, adding the same amount of VOT at a time, the plosive is eventually a strongly aspirated voiceless bilabial .
For instance using the example of the dog went home, a person suffering from expressive aphasia or Parkinson's disease would say the dog goed home.
''Behavioral responses may reflect late, conscious processes and be affected by other systems such as orthography, and thus they may mask speaker's ability to recognize sounds based on lower-level acoustic distributions.''
Many psychologists relate Parkinson's disease to Progressive Nonfluent Aphasia, which would cause a person to have comprehension deficits and being able to recognize irregular verbs.
(Such a continuum was used in an experiment by Lisker and Abramson in 1970.
This pathway includes the sylvian parietotemporal, inferior frontal gyrus, anterior insula, and premotor cortex.
These patients are described with having severe syntactical deficits, which means that they have extreme difficulty in forming sentences correctly.
The experimenters then conducted the same test and found that the people still had more of an advantage of audio only over visual only, but they also found that the subjects did better in audio-visual than audio alone.
The term 'speech perception' describes the process of interest that employs sub lexical contexts to the probe process.
The results of the experiment showed that listeners grouped sounds into discrete categories, even though the sounds they were hearing were varying continuously.
Over time the symptoms go from mild to severe, which can cause extreme difficulties in a person's life.
It utilizes a vertical processing mechanism where limited stimuli are processed by special-purpose areas of the brain that are stimuli specific.
The exemplar-based approaches claim listeners store information for both word- and talker-recognition.
Another major source of variation is articulatory carefulness vs. sloppiness which is typical for connected speech (articulatory "undershoot" is obviously reflected in the acoustic properties of the sounds produced).
Since there is no cure for it, the patient will probably end up having to have surgery done to relieve some of the symptoms.
Perceptual constancy is a phenomenon not specific to speech perception only; it exists in other types of perception too.
By claiming that the actual articulatory gestures that produce different speech sounds are themselves the units of speech perception, the theory bypasses the problem of lack of invariance.
Phonetic environment affects the acoustic properties of speech sounds.
In such an experiment, a baby is sucking a special nipple while presented with sounds.
In a classic experiment, Richard M. Warren (1970) replaced one phoneme of a word with a cough-like sound.
''At first glance, the solution to the problem of how we perceive speech seems deceptively simple.
The effect of receptive aphasia on understanding is much more severe.
This model has drastically changed from how psychologists look at perception.
The methods used in speech perception research can be roughly divided into three groups: behavioral, computational, and, more recently, neurophysiological methods.
When a human and a non-human sound is played, babies turn their head only to the source of human sound.
For example, if two foreign-language sounds are assimilated to a single mother-tongue category the difference between them will be very difficult to discern.
Others even claim that certain sound categories are innate, that is, they are genetically specified (see discussion about innate vs. acquired categorical distinctiveness).
in 2006 showed that 8-year-olds who were given six months of musical training showed an increase in both their pitch detection performance and their electrophysiological measures when made to listen to an unknown foreign language.
The fuzzy logical theory of speech perception developed by Dominic Massaro proposes that people remember speech sounds in a probabilistic, or graded, way.
A second study, performed in 2006 on a group of English speakers and 3 groups of East Asian students at University of Southern California, discovered that English speakers who had begun musical training at or before age 5 had an 8% chance of having perfect pitch.
The perceptual space between categories is therefore warped, the centers of categories (or "prototypes") working like a sieve or like magnets for incoming speech sounds.
The tritone paradox is where a listener is presented with two computer-generated tones (such as C and F-Sharp) that are half an octave (or a tritone) apart and are then asked to determine whether the pitch of the sequence is descending or ascending.
Vocal-tract-size differences result in formant-frequency variation across speakers; therefore a listener has to adjust his/her perceptual system to the acoustic characteristics of a particular speaker.
For instance, the English consonant  may vary in its acoustic details across different phonetic contexts (see above), yet all 's as perceived by a listener fall within one category (voiced alveolar plosive) and that is because "linguistic representations are abstract, canonical, phonetic segments or the gestures that underlie these segments".
Listeners perceive gestures not by means of a specialized decoder (as in the Motor Theory) but because information in the acoustic signal specifies the gestures that form it.
The theory is closely related to the modularity hypothesis, which proposes the existence of a special-purpose module, which is supposed to be innate and probably human-specific.
Research into speech perception (SP) has by no means explained every aspect of the processes involved.
Two Versions of Speech Mode Hypothesis
It consists of many different language and grammatical functions, such as: features, segments (phonemes), syllabic structure (unit of pronunciation), phonological word forms (how sounds are grouped together), grammatical features, morphemic (prefixes and suffixes), and semantic information (the meaning of the words).
They found that the symptoms were still present but the patients were more aware of their difficulties than before they had surgery.
Research in how people with language or hearing impairment perceive speech is not only intended to discover possible treatments.
One example is the tritone paradox.
The first section of the Dual Stream Model is the ventral pathway.
However, these systems often do poorly in more realistic listening situations where humans can understand speech without difficulty.
Computer models have been used to address several questions in speech perception, including how the sound signal itself is processed to extract the acoustic cues used in speech, and how speech information is used for higher-level processes, such as word recognition.
According to this theory, particular instances of speech sounds are stored in the memory of a listener.
Supporting this theory are several experiments reported by Johnson that suggest that our signal identification is more accurate when we are familiar with the talker or when we have visual representation of the talker's gender.
Perception accuracy usually drops in the latter condition.
Among the new methods (see Research methods below) that help us to study speech perception, near-infrared spectroscopy is widely used in infants.
For example, the subject may not show sensitivity to the difference between two speech sounds in a discrimination test, but brain responses may reveal sensitivity to these differences.
Some experts even argue that duration can help in distinguishing of what is traditionally called short and long vowels in English.
This pathway incorporates middle temporal gyrus, inferior temporal sulcus and perhaps the inferior temporal gyrus.
For instance using the example of the dog went home, a person suffering from expressive aphasia or Parkinson's disease would say the dog goed home.
Though they have difficulty saying things or describing things, these people showed that they could do well in online comprehension tasks.
After obtaining at least a fundamental piece of information about phonemic structure of the perceived entity from the acoustic signal, listeners can compensate for missing or noise-masked phonemes using their knowledge of the spoken language.
Originally it was theorized that the neural signals for music were processed in a specialized "module" in the right hemisphere of the brain.
Garnes and Bond (1976) also used carrier sentences when researching the influence of semantic knowledge on perception.
Methods used to measure neural responses to speech include event-related potentials, magnetoencephalography, and near infrared spectroscopy.
It has been proposed that this is achieved by means of the perceptual normalization process in which listeners filter out the noise (i.e.
Thus proving that given the right conditions, it is possible to prevent infants' loss of the ability to distinguish speech sounds in languages other than those found in the native language.
variation) to arrive at the underlying category.
Speech perception research has applications in building computer systems that can recognize speech, in improving speech recognition for hearing- and language-impaired listeners, and in foreign-language teaching.
Computer models of the fuzzy logical theory have been used to demonstrate that the theory's predictions of how speech sounds are categorized correspond to the behavior of human listeners.
Then a stimulus is played repeatedly.
Speech sounds do not strictly follow one another, rather, they overlap.
More recent research using different tasks and methods suggests that listeners are highly sensitive to acoustic differences within a single phonetic category, contrary to a strict categorical account of speech perception.
Or, the VOT values marking the boundary between voiced and voiceless plosives are different for labial, alveolar and velar plosives and they shift under stress or depending on the position within a syllable.
Research into the relationship between music and cognition is an emerging field related to the study of speech perception.
This disease attacks the brain and makes the patients unable to stop shaking.
One important factor that causes variation is differing speech rate.
In this continuum of, for example, seven sounds, native English listeners will identify the first three sounds as  and the last three sounds as  with a clear boundary between the two categories.
Its primary function is to take the sensory or phonological stimuli and transfer it into an articulatory-motor representation (formation of speech).
If one could identify stretches of the acoustic waveform that correspond to units of perception, then the path from sound to meaning would be clear.
It is agreed upon, that aphasics suffer from perceptual deficits.
Several months following implantation, children with cochlear implants can normalize speech perception.
The first ever hypothesis of speech perception was used with patients who acquired an auditory comprehension deficit, also known as receptive aphasia.
It suggests that people remember descriptions of the perceptual units of language, called prototypes.
'''Speech perception''' is the process by which the sounds of language are heard, interpreted and understood.
: Listening to speech engages specialized speech mechanisms for perceiving speech.
When the talker is unpredictable or the sex misidentified, the error rate in word-identification is much higher.
Cochlear implantation restores access to the acoustic signal in individuals with sensorineural hearing loss.
When a patient has this procedure done, they are most likely going to receive a deep brain stimulation.
For the East Asian students who were fluent in their native tonal language, 92 percent of the students had perfect pitch.
This influence can even be exerted at a distance of two or more segments (and across syllable- and word-boundaries).
The exemplar models have to face several objections, two of which are (1) insufficient memory capacity to store every utterance ever heard and, concerning the ability to produce what was heard, (2) whether also the talker's own articulatory gestures are stored or computed when producing utterances that would sound as the auditory memories.
The resulting acoustic structure of concrete speech productions depends on the physical and psychological properties of individual speakers.
Through the research in these categories it has been found that there may not be a specific speech mode but instead one for auditory codes that require complicated auditory processing.
These types of experiments help to provide a basic description of how listeners perceive and categorize speech sounds.
Behavioral experiments are based on an active role of a participant, i.e.

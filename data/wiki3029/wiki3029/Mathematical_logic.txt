''Principia Mathematica'' is considered one of the most influential works of the 20th century, although the framework of type theory did not prove popular as a foundational theory for mathematics (Ferreirós 2001, p.&nbsp;445).
The borderlines amongst these fields, and the lines separating mathematical logic and other fields of mathematics, are not always sharp.
An early proponent of predicativism was Hermann Weyl, who showed it is possible to develop a large part of real analysis using only predicative methods (Weyl 1918).
More advanced results concern the structure of the Turing degrees and the lattice of recursively enumerable sets.
The first half of the 20th century saw an explosion of fundamental results, accompanied by vigorous debate over the foundations of mathematics.
Results such as the Gödel–Gentzen negative translation show that it is possible to embed (or ''translate'') classical logic into intuitionistic logic, allowing some properties about intuitionistic proofs to be transferred back to classical proofs.
Lindström's theorem implies that the only extension of first-order logic satisfying both the compactness theorem and the Downward Löwenheim–Skolem theorem is first-order logic.
Because of its applicability in diverse fields of mathematics, mathematicians including Saunders Mac Lane have proposed category theory as a foundational system for mathematics, independent of set theory.
The theory of semantics of programming languages is related to model theory, as is program verification (in particular, model checking).
The success in axiomatizing geometry motivated Hilbert to seek complete axiomatizations of other areas of mathematics, such as the natural numbers and the real line.
The system of Kripke–Platek set theory is closely related to generalized recursion theory.
Here a theory is a set of formulas in a particular formal logic and signature, while a model is a structure that gives a concrete interpretation of the theory.
This theorem, known as the Banach–Tarski paradox, is one of many counterintuitive results of the axiom of choice.
Numerous results in recursion theory were obtained in the 1940s by Stephen Cole Kleene and Emil Leon Post.
Classical recursion theory focuses on the computability of functions from the natural numbers to the natural numbers.
Cesare Burali-Forti (1897) was the first to state a paradox: the Burali-Forti paradox shows that the collection of all ordinal numbers cannot form a set.
The fundamental results establish a robust, canonical class of computable functions with numerous independent, equivalent characterizations using Turing machines, &lambda; calculus, and other systems.
Over the next twenty years, Cantor developed a theory of transfinite numbers in a series of publications.
Another type of logics are s that allow inductive definitions, like one writes for primitive recursive functions.
This leaves open the possibility of consistency proofs that cannot be formalized within the system they consider.
The word problem for groups was proved algorithmically unsolvable by Pyotr Novikov in 1955 and independently by W. Boone in 1959.
In this logic, quantifiers may only be nested to finite depths, as in first-order logic, but formulas may have finite or countably infinite conjunctions and disjunctions within them.
It includes the study of computability in higher types as well as areas such as hyperarithmetical theory and &alpha;-recursion theory.
Peano was unaware of Frege's work at the time.
When these definitions were shown equivalent to Turing's formalization involving Turing machines, it became clear that a new concept &ndash; the computable function &ndash; had been discovered, and that this definition was robust enough to admit numerous independent characterizations.
With the advent of the BHK interpretation and Kripke models, intuitionism became easier to reconcile with classical mathematics.
The systems of propositional logic and first-order logic are the most widely studied today, because of their applicability to foundations of mathematics and because of their desirable proof-theoretic properties.
It states that given a collection of nonempty sets there is a single set ''C'' that contains exactly one element from each set in the collection.
Although Kronecker's argument was carried forward by constructivists in the 20th century, the mathematical community as a whole rejected them.
The study of computability came to be known as recursion theory, because early formalizations by Gödel and Kleene relied on recursive definitions of functions.
The immediate criticism of the method led Zermelo to publish a second exposition of his result, directly addressing criticisms of his proof (Zermelo 1908a).
The study of constructive mathematics includes many different programs with various definitions of ''constructive''.
The first significant result in this area, Fagin's theorem (1974) established that NP is precisely the set of languages expressible by sentences of existential second-order logic.
Previous conceptions of a function as a rule for computation, or a smooth graph, were no longer adequate.
''Determinacy'' refers to the possible existence of winning strategies for certain two-player games (the games are said to be ''determined'').
The compactness theorem first appeared as a lemma in Gödel's proof of the completeness theorem, and it took many years before logicians grasped its significance and began to apply it routinely.
These texts, written in an austere and axiomatic style, emphasized rigorous presentation and set-theoretic foundations.
Kleene (1943) introduced the concepts of relative computability, foreshadowed by Turing (1939), and the arithmetical hierarchy.
Gödel (1958) gave a different consistency proof, which reduces the consistency of classical arithmetic to that of intutitionistic arithmetic in higher types.
Georg Cantor developed the fundamental concepts of infinite set theory.
Two famous statements in set theory are the axiom of choice and the continuum hypothesis.
The existence of the smallest large cardinal typically studied, an inaccessible cardinal, already implies the consistency of ZFC.
Mathematical logic emerged in the mid-19th century as a subfield of mathematics independent of the traditional study of logic (Ferreirós 2001, p.&nbsp;443).
In 1858, Dedekind proposed a definition of the real numbers in terms of Dedekind cuts of rational numbers (Dedekind 1872), a definition still employed in contemporary texts.
In addition to removing ambiguity from previously naive terms such as function, it was hoped that this axiomatization would allow for consistency proofs.
The first two of these were to resolve the continuum hypothesis and prove the consistency of elementary arithmetic, respectively; the tenth was to produce a method that could decide whether a multivariate polynomial equation over the integers has a solution.
Here a logical system is said to be effectively given if it is possible to decide, given any formula in the language of the system, whether the formula is an axiom, and one which can express the Peano axioms is called "sufficiently strong."
Leopold Kronecker famously stated "God made the integers; all else is the work of man," endorsing a return to the study of finite, concrete objects in mathematics.
The logics studied before the development of first-order logic, for example Frege's logic, had similar set-theoretic aspects.
While the ability to make such a choice is considered obvious by some, since each set in the collection is nonempty, the lack of a general, concrete rule by which the choice can be made renders the axiom nonconstructive.
Zermelo's axioms incorporated the principle of limitation of size to avoid Russell's paradox.
The semantics are defined so that, rather than having a separate domain for each higher-type quantifier to range over, the quantifiers instead range over all objects of the appropriate type.
Around the same time Richard Dedekind showed that the natural numbers are uniquely characterized by their induction properties.
The algorithmic unsolvability of the problem was proved by Yuri Matiyasevich in 1970 (Davis 1973).
(He also noted that his methods were equally applicable to algebraically closed fields of arbitrary characteristic.)
The use of infinitesimals, and the very definition of function, came into question in analysis, as pathological examples such as Weierstrass' nowhere-differentiable continuous function were discovered.
Recent work along these lines has been conducted by W. Hugh Woodin, although its importance is not yet clear (Woodin 2001).
More limited versions of constructivism limit themselves to natural numbers, number-theoretic functions, and sets of natural numbers (which can be used to represent real numbers, facilitating the study of mathematical analysis).
Contemporary research in recursion theory includes the study of applications such as algorithmic randomness, computable model theory, and reverse mathematics, as well as new results in pure recursion theory.
Cauchy in 1821 defined continuity in terms of infinitesimals (see Cours d'Analyse, page 34).
Among these is the theorem that a line contains at least two points, or that circles of the same radius whose centers are separated by that radius must intersect.
Many of the basic notions, such as ordinal and cardinal numbers, were developed informally by Cantor before formal axiomatizations of set theory were developed.
Later, Kleene and Kreisel would study formalized versions of intuitionistic logic (Brouwer rejected formalization, and presented his work in unformalized natural language).
This would prove to be a major area of research in the first half of the 20th century.
Terminology coined by these texts, such as the words ''bijection'', ''injection'', and ''surjection'', and the set-theoretic foundations the texts employed, were widely adopted throughout mathematics.
The axiom of choice, first stated by Zermelo (1904), was proved independent of ZF by Fraenkel (1922), but has come to be widely accepted by mathematicians.
Later work by Paul Cohen (1966) showed that the addition of urelements is not needed, and the axiom of choice is unprovable in ZF.
The first results about unsolvability, obtained independently by Church and Turing in 1936, showed that the Entscheidungsproblem is algorithmically unsolvable.
Algebraic logic uses the methods of abstract algebra to study the semantics of formal logics.
Gödel's completeness theorem (Gödel 1929) established the equivalence between semantic and syntactic definitions of logical consequence in first-order logic.
In 1910, the first volume of ''Principia Mathematica'' by Russell and Alfred North Whitehead was published.
This study began in the late 19th century with the development of axiomatic frameworks for geometry, arithmetic, and analysis.
Gentzen's result introduced the ideas of cut elimination and proof-theoretic ordinals, which became key tools in proof theory.
Higher-order logics allow for quantification not only of elements of the domain of discourse, but subsets of the domain of discourse, sets of such subsets, and other objects of higher type.
Model theory is closely related to universal algebra and algebraic geometry, although the methods of model theory focus more on logical considerations than those fields.
The method of forcing is employed in set theory, model theory, and recursion theory, as well as in the study of intuitionistic mathematics.
Kleene later generalized recursion theory to higher-order functionals.
Mathematicians such as Karl Weierstrass began to construct functions that stretched intuition, such as nowhere-differentiable continuous functions.
Leopold Löwenheim (1915) and Thoralf Skolem (1920) obtained the Löwenheim–Skolem theorem, which says that first-order logic cannot control the cardinalities of infinite structures.
Skepticism about the axiom of choice was reinforced by recently discovered paradoxes in naive set theory.
These foundations use toposes, which resemble generalized models of set theory that may employ classical or nonclassical logic.
It was shown that Euclid's axioms for geometry, which had been taught for centuries as an example of the axiomatic method, were incomplete.
Morley's categoricity theorem, proved by Michael D. Morley (1965), states that if a first-order theory in a countable language is categorical in some uncountable cardinality, i.e.
This idea led to the study of proof theory.
In the early 20th century, Luitzen Egbertus Jan Brouwer founded intuitionism as a philosophy of mathematics.
The continuum hypothesis, first proposed as a conjecture by Cantor, was listed by David Hilbert as one of his 23 problems in 1900.
Because proofs are entirely finitary, whereas truth in a structure is not, it is common for work in constructive mathematics to emphasize provability.
Frege's work remained obscure, however, until Bertrand Russell began to promote it near the turn of the century.
'''Mathematical logic''' is a subfield of mathematics exploring the applications of formal logic to mathematics.
The most well studied infinitary logic is .
Moreover, Hilbert proposed that the analysis should be entirely concrete, using the term ''finitary'' to refer to the methods he would allow but not precisely defining them.
Mathematicians began to search for axiom systems that could be used to formalize large parts of mathematics.
David Hilbert argued in favor of the study of the infinite, saying "No one shall expel us from the Paradise that Cantor has created."
The study of computability theory in computer science is closely related to the study of computability in mathematical logic.
The existence of these strategies implies structural properties of the real line and other Polish spaces.
Dedekind (1888) proposed a different characterization, which lacked the formal logical character of Peano's axioms.
Each area has a distinct focus, although many techniques and results are shared among multiple areas.
These axioms, together with the additional axiom of replacement proposed by Abraham Fraenkel, are now called Zermelo–Fraenkel set theory (ZF).
The resulting structure, a model of elliptic geometry, satisfies the axioms of plane geometry except the parallel postulate.
Recursion theory grew from the work of Alonzo Church and Alan Turing in the 1930s, which was greatly extended by Kleene and Post in the 1940s.
The Curry&ndash;Howard isomorphism between proofs and programs relates to proof theory, especially intuitionistic logic.
Stefan Banach and Alfred Tarski (1924) showed that the axiom of choice can be used to decompose a solid ball into a finite number of pieces which can then be rearranged, with no scaling, to make two solid balls of the original size.
For example, any provably total function in intuitionistic arithmetic is computable; this is not true in classical theories of arithmetic such as Peano arithmetic.
A second thread in the history of foundations of mathematics involves nonclassical logics and constructive mathematics.
This result, known as Gödel's incompleteness theorem, establishes severe limitations on axiomatic foundations for mathematics, striking a strong blow to Hilbert's program.
This paper led to the general acceptance of the axiom of choice in the mathematics community.
In logic, the term ''arithmetic'' refers to the theory of the natural numbers.
Stronger logics, such as first-order logic and higher-order logic, are studied using more complicated algebraic structures such as cylindric algebras.
Theories of logic were developed in many cultures in history, including China, India, Greece and the Islamic world.
The first such axiomatization, due to Zermelo (1908b), was extended slightly to become Zermelo–Fraenkel set theory (ZF), which is now the most widely used foundational theory for mathematics.
Zermelo (1908b) provided the first set of axioms for set theory.
The study of '''constructive mathematics''', in the context of mathematical logic, includes the study of systems in non-classical logic such as intuitionistic logic, as well as the study of predicative systems.
Computer science also contributes to mathematics by developing techniques for the automatic checking or even finding of proofs, such as automated theorem proving and logic programming.
His early results developed the theory of cardinality and proved that the reals and the natural numbers have different cardinalities (Cantor 1874).
Very soon thereafter, Bertrand Russell discovered Russell's paradox in 1901, and Jules Richard (1905) discovered Richard's paradox.
Despite the fact that large cardinals have extremely high cardinality, their existence has many ramifications for the structure of the real line.
Gentzen showed that it is possible to produce a proof of the consistency of arithmetic in a finitary system augmented with axioms of transfinite induction, and the techniques he developed to do so were seminal in proof theory.
In his work on the incompleteness theorems in 1931, Gödel lacked a rigorous concept of an effective formal system; he immediately realized that the new definitions of computability could be used for this purpose, allowing him to state the incompleteness theorems in generality that could only be implied in the original paper.
The Löwenheim–Skolem theorem (1919) showed that if a set of sentences in a countable first-order language has an infinite model then it has at least one model of each infinite cardinality.
In addition to the independence of the parallel postulate, established by Nikolai Lobachevsky in 1826 (Lobachevsky 1840), mathematicians discovered that certain theorems taken for granted by Euclid were not in fact provable from his axioms.
The 19th century saw great advances in the theory of real analysis, including theories of convergence of functions and Fourier series.
This philosophy, poorly understood at first, stated that in order for a mathematical statement to be true to a mathematician, that person must be able to ''intuit'' the statement, to not only believe its truth but understand the reason for its truth.
Gödel used the completeness theorem to prove the compactness theorem, demonstrating the finitary nature of first-order logical consequence.
The two-dimensional notation Frege developed was never widely adopted and is unused in contemporary texts.
In 1900, Hilbert posed a famous list of 23 problems for the next century.
One can formally define an extension of first-order logic &mdash; a notion which encompasses all logics in this section because they behave like first-order logic in certain fundamental ways, but does not encompass all logics in general, e.g.
all models of this cardinality are isomorphic, then it is categorical in all uncountable cardinalities.
In 18th-century Europe, attempts to treat the operations of formal logic in a symbolic or algebraic way had been made by philosophical mathematicians including Leibniz and Lambert, but their labors remained isolated and little known.
Kleene's work with the proof theory of intuitionistic logic showed that constructive information can be recovered from intuitionistic proofs.
The method of quantifier elimination can be used to show that definable sets in particular theories cannot be too complicated.
Fraenkel (1922) proved that the axiom of choice cannot be proved from the remaining axioms of Zermelo's set theory with urelements.
Beginning in 1935, a group of prominent mathematicians collaborated under the pseudonym Nicolas Bourbaki to publish a series of encyclopedic mathematics texts.
Hilbert, however, did not acknowledge the importance of the incompleteness theorem for some time.
In his doctoral thesis, Kurt Gödel (1929) proved the completeness theorem, which establishes a correspondence between syntax and semantics in first-order logic.
Although higher-order logics are more expressive, allowing complete axiomatizations of structures such as the natural numbers, they do not satisfy analogues of the completeness and compactness theorems from first-order logic, and are thus less amenable to proof-theoretic analysis.
These areas share basic results on logic, particularly first-order logic, and definability.
Contemporary research in set theory includes the study of large cardinals and determinacy.
The unifying themes in mathematical logic include the study of the expressive power of formal systems and the deductive power of formal proof systems.
Cantor's study of arbitrary infinite sets also drew criticism.
Stronger classical logics such as second-order logic or infinitary logic are also studied, along with nonclassical logics such as intuitionistic logic.
Several deduction systems are commonly considered, including Hilbert-style deduction systems, systems of natural deduction, and the sequent calculus developed by Gentzen.
Hilbert's tenth problem asked for an algorithm to determine whether a multivariate polynomial equation with integer coefficients has a solution in the integers.
This independence result did not completely settle Hilbert's question, however, as it is possible that new axioms for set theory could resolve the hypothesis.
Brouwer's philosophy was influential, and the cause of bitter disputes among prominent mathematicians.
Large cardinals are cardinal numbers with particular properties so strong that the existence of such cardinals cannot be proved in ZFC.
Recent developments in proof theory include the study of proof mining by Ulrich Kohlenbach and the study of proof-theoretic ordinals by Michael Rathjen.
Gödel's incompleteness theorem marks not only a milestone in recursion theory and proof theory, but has also led to Löb's theorem in modal logic.
'''Recursion theory''', also called '''computability theory''', studies the properties of computable functions and the Turing degrees, which divide the uncomputable functions into sets that have the same level of uncomputability.
Thus, for example, it is possible to say that an object is a whole number using a formula of  such as
From 1890 to 1905, Ernst Schröder published ''Vorlesungen über die Algebra der Logik'' in three volumes.
At the most accommodating end, proofs in ZF set theory that do not use the axiom of choice are called constructive by many mathematicians.
The relationship between provability in classical (or nonconstructive) systems and provability in intuitionistic (or constructive, respectively) systems is of particular interest.
Other formalizations of set theory have been proposed, including von Neumann–Bernays–Gödel set theory (NBG), Morse–Kelley set theory (MK), and New Foundations (NF).
Contemporary work in the foundations of mathematics often focuses on establishing which parts of mathematics can be formalized in particular formal systems (as in reverse mathematics) rather than trying to find theories in which all of mathematics can be developed.
A fundamental example is the use of Boolean algebras to represent truth values in classical propositional logic, and the use of Heyting algebras to represent truth values in intuitionistic propositional logic.
As the goal of early foundational studies was to produce axiomatic theories for all parts of mathematics, this limitation was particularly stark.
Computer scientists often focus on concrete programming languages and feasible computability, while researchers in mathematical logic often focus on computability as a theoretical concept and on noncomputability.
Many special cases of this conjecture have been established.
The mathematical field of category theory uses many formal axiomatic methods, and includes the study of categorical logic, but category theory is not ordinarily considered a subfield of mathematical logic.
Gottlob Frege presented an independent development of logic with quantifiers in his ''Begriffsschrift'', published in 1879, a work generally considered as marking a turning point in the history of logic.
Of these, ZF, NBG, and MK are similar in describing a cumulative hierarchy of sets.
Giuseppe Peano (1889) published a set of axioms for arithmetic that came to bear his name (Peano axioms), using a variation of the logical system of Boole and Schröder but adding quantifiers.
In 1931, Gödel published ''On Formally Undecidable Propositions of Principia Mathematica and Related Systems'', which proved the incompleteness (in a different meaning of the word) of all sufficiently strong, effective first-order theories.
Subsequent work to resolve these problems shaped the direction of mathematical logic, as did the effort to resolve Hilbert's ''Entscheidungsproblem'', posed in 1928.
There is a difference of emphasis, however.
it does not encompass intuitionistic, modal or fuzzy logic.
In 1891, he published a new proof of the uncountability of the real numbers that introduced the diagonal argument, and used this method to prove Cantor's theorem that no set can have the same cardinality as its powerset.
Hilbert (1899) developed a complete set of axioms for geometry, building on previous work by Pasch (1882).
Vaught's conjecture, named after Robert Lawson Vaught, says that this is true even independently of the continuum hypothesis.
It says that a set of sentences has a model if and only if every finite subset has a model, or in other words that an inconsistent set of formulas must have a finite inconsistent subset.
Early results from formal logic established limitations of first-order logic.
A consequence of this definition of truth was the rejection of the law of the excluded middle, for there are statements that, according to Brouwer, could not be claimed to be true while their negations also could not be claimed true.
In the mid-19th century, flaws in Euclid's axioms for geometry became known (Katz 1998, p.&nbsp;774).
Generalized recursion theory extends the ideas of recursion theory to computations that are no longer necessarily finite.
This problem asked for a procedure that would decide, given a formalized mathematical statement, whether the statement is true or false.
A modern subfield developing from this is concerned with o-minimal structures.
Kleene and Kreisel studied formal versions of intuitionistic mathematics, particularly in the context of proof theory.
When applied to first-order logic, the first incompleteness theorem implies that any sufficiently strong, consistent, effective first-order theory has models that are not elementarily equivalent, a stronger limitation than the one established by the Löwenheim–Skolem theorem.
'''Proof theory''' is the study of formal proofs in various logical deduction systems.
Gödel showed that the continuum hypothesis cannot be disproven from the axioms of Zermelo–Fraenkel set theory (with or without the axiom of choice), by developing the constructible universe of set theory in which the continuum hypothesis must hold.
Intuitionistic logic specifically does not include the law of the excluded middle, which states that each sentence is either true or its negation is true.
Results of Kurt Gödel, Gerhard Gentzen, and others provided partial resolution to the program, and clarified the issues involved in proving consistency.
Since its inception, mathematical logic has both contributed to, and has been motivated by, the study of foundations of mathematics.
There are many known examples of undecidable problems from ordinary mathematics.
The set of all models of a particular theory is called an elementary class; classical model theory seeks to determine the properties of models in a particular elementary class, or determine whether certain classes of structures form elementary classes.
The '''second incompleteness theorem''' states that no sufficiently strong, consistent, effective axiom system for arithmetic can prove its own consistency, which has been interpreted to show that Hilbert's program cannot be completed.
These systems, though they differ in many details, share the common property of considering only expressions in a fixed formal language.
'''Set theory''' is the study of sets, which are abstract collections of objects.
Gödel's theorem shows that a consistency proof of any sufficiently strong, effective axiom system cannot be obtained in the system itself, if the system is consistent, nor in any weaker system.
Intuitionistic logic was developed by Heyting to study Brouwer's program of intuitionism, in which Brouwer himself avoided formalization.
Work in set theory showed that almost all ordinary mathematics can be formalized in terms of sets, although there are some theorems that cannot be proven in common axiom systems for set theory.
In the 19th century, mathematicians became aware of logical gaps and inconsistencies in their field.
These proofs are represented as formal mathematical objects, facilitating their analysis by mathematical techniques.
Weierstrass began to advocate the arithmetization of analysis, which sought to axiomatize analysis using properties of the natural numbers.
The set ''C'' is said to "choose" one element from each set in the collection.
Cohen's proof developed the method of forcing, which is now an important tool for establishing independence results in set theory.
The '''first incompleteness theorem''' states that for any consistent, effectively given (defined below) logical system that is capable of interpreting arithmetic (that is, of expressing the Peano axioms) there exists a statement (the Gödel sentence) which is true (in the sense that it holds for the natural numbers) but not provable within that logical system (and which indeed may fail in some non-standard models of arithmetic which may be consistent with the logical system.)
The completeness and compactness theorems allow for sophisticated analysis of logical consequence in first-order logic and the development of model theory, and they are a key reason for the prominence of first-order logic in mathematics.
Dedekind's work, however, proved theorems inaccessible in Peano's system, including the uniqueness of the set of natural numbers (up to isomorphism) and the  recursive definitions of addition and multiplication from the successor function and mathematical induction.
Tarski (1948) established quantifier elimination for real-closed fields, a result which also shows the theory of the field of real numbers is decidable.

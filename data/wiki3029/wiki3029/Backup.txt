; Backup validation : (also known as "backup success validation") Provides information about the backup, and proves compliance to regulatory bodies outside the organization: for example, an insurance company in the USA might be required under HIPAA to demonstrate that its client data meet records retention requirements.
Examples of programs that use this method are rdiff-backup and Time Machine.
The use of an auto-changer or jukebox can make optical discs a feasible option for larger-scale backup systems.
This type of storage is very convenient and speedy, but is relatively expensive.
Importantly a data replica ''can'' be off-site but also ''on-line'' (e.g., an off-site RAID mirror).
Imaging is generally more useful for deploying a standard configuration to many systems rather than as a tool for making ongoing backups of diverse systems.
At this point the snapshot can be backed up through normal methods.
This represents a challenge when backing up a file that is constantly changing.
; Reporting: In larger configurations, reports are useful for monitoring media usage, device status, errors, vault coordination and other information about the backup process.
; Backup window: The period of time when backups are permitted to run on a system is called the backup window.
Encryption is a CPU intensive process that can slow down backup speeds, and the security of the encrypted backups is only as effective as the security of the key management policy.
Some optical storage systems allow for cataloged data backups without human contact with the discs, allowing for longer data integrity.
and the dates they were produced.
; Optical storage : Recordable CDs, DVDs, and Blu-ray Discs are commonly used with personal computers and generally have low media unit costs.
Other organizations contract this out to a third-party recovery center.
Some backup software looks at the date of the file and compares it with the last backup to determine whether the file was changed.
The process usually involves unmounting the filesystem and running a program like dd (Unix).
Data loss can be a common experience of computer users; a 2008 survey found that 66% of respondents had lost files on their home PC.
The scales may be very different, but the objectives and limitations are essentially the same.
; On-line : On-line backup storage is typically the most accessible type of data storage, which can begin restore in milliseconds of time.
Though backups represent a simple form of disaster recovery, and should be part of any disaster recovery plan, backups by themselves should not be considered a complete disaster recovery plan.
Backups must be performed in a manner that does not compromise the original owner's undertaking.
External disks can be connected via local interfaces like SCSI, USB, FireWire, or eSATA, or via longer distance technologies like Ethernet, iSCSI, or Fibre Channel.
This can be achieved with data encryption and proper media handling policies.
; Off-site data protection: To protect against a disaster or other site-specific problem, many people choose to send backup media to an off-site vault.
; Snapshot backup: A snapshot is an instantaneous function of some storage systems that presents a copy of the file system as if it were frozen at a specific point in time, often by a copy-on-write mechanism.
It is also useful to save metadata that describes the computer or the filesystem being backed up.
Other variations of incremental backup include multi-level incrementals and incremental backups that compare parts of files instead of just the whole file.
The cost of commercial backup software can also be considerable.
After the full backup is performed, the system will periodically synchronize the full backup with the live copy, while storing the data necessary to reconstruct older versions.
Because a DR site is itself a huge investment, backing up is very rarely considered the preferred method of moving data to a DR site.
; Off-line : Off-line storage requires some direct human action to provide access to the storage media: for example inserting a tape into a tape drive or plugging in a cable.
If a file is open, the contents on disk may not correctly represent what the owner of the file intends.
; Remote backup service : As broadband Internet access becomes more widespread, remote backup services are gaining in popularity.
Making a more recent recovery point achievable requires increasing the frequency of synchronization between the source data and the backup repository.
Different approaches have different advantages.
; Identification of changes: Some filesystems have an archive bit for each file that says it was recently changed.
; Backup site or disaster recovery center (DR center): In the event of a disaster, the data on backup media will not be sufficient to recover.
While a snapshot is very handy for viewing a filesystem as it was at a different point in time, it is hardly an effective backup mechanism by itself.
For example, for the period of time that a computer system is being backed up, the hard drive is busy reading files for the purpose of backing up, and its full bandwidth is no longer available for other tasks.
A more typical way would be remote disk mirroring, which keeps the DR data as up to date as possible.
The secondary purpose of backups is to recover data from an earlier time, according to a user-defined data retention policy, typically configured within a backup application for how long copies of data are required.
; Performance impact: All backup schemes have some performance impact on the system being backed up.
This is typically the time when the system sees the least usage and the backup process will have the least amount of interference with normal operations.
These include optimizations for dealing with open files and live data sources as well as compression, encryption, and de-duplication, among others.
; Monitored backup: Backup processes are monitored by a third party monitoring center, which alerts users to any errors that occur during automated backups.
A successful backup job starts with selecting and extracting coherent units of data.
; Deduplication : When multiple similar systems are backed up to the same destination storage device, there exists the potential for much redundancy within the backed up data.
Restoring an entire system would require starting from the most recent full backup and then applying just the last differential backup since the last full backup.
Backing up via the Internet to a remote location can protect against some worst-case scenarios such as fires, floods, or earthquakes which would destroy any backups in the immediate vicinity along with everything else.
;Duplication : Sometimes backup jobs are duplicated to a second set of storage media.
However, there have been cases where conflicting definitions of these terms have been used.
Restoring the whole system to the date of the last incremental backup would require starting from the last full backup taken before the data loss, and then applying in turn each of the incremental backups since then.
The backup window is usually planned with users' convenience in mind.
; Deleted files : To prevent the unintentional restoration of files that have been intentionally deleted, a record of the deletion must be kept.
; Partial file copying: Instead of copying whole files, one can limit the backup to only the blocks or bytes within a file that have changed in a given period of time.
This is generally done by saving byte or block-level differences rather than file-level differences.
Essentially, this is the roll-back that will be experienced as a result of the recovery.
Some filesystems, such as XFS, provide a "dump" utility that reads the disk sequentially for high performance while skipping unused sections.
There are many formats, many of which are proprietary or specific to certain markets like mainframes or a particular brand of personal computer.
This technique can use substantially less storage space on the backup medium, but requires a high level of sophistication to reconstruct files in a restore situation.
Establishing a chain of trusted individuals (and vendors) is critical to defining the security of the data.
Many different techniques have been developed to optimize the backup procedure.
Matching the correct amount of storage capacity (over time) with the backup needs is an important part of the design of a backup scheme.
This usually includes an inconsistent image of the data files plus a log of changes made while the procedure is running.
One disadvantage, compared to the incremental backup method, is that as time from the last full backup (and thus the accumulated changes in data) increases, so does the time to perform the differential backup.
Secondly, users must trust a third party service provider to maintain the privacy and integrity of their data, although confidentiality can be assured by encrypting the data before transmission to the backup service with an encryption key known only to the user.
Generally this gives access to any previous version, all the way back to the file's creation time.
This can either be done using hard links, or using binary diffs.
It can also serve as a centralized location for applying other data manipulation techniques.
Some organizations have their own data recovery centers that are equipped for this scenario.
Additionally, some backup systems can reorganize the repository to synthesize full backups from a series of incrementals.
File locking is useful for regulating access to open files.
There are also many different ways in which these devices can be arranged to provide geographic redundancy, data security, and portability.
If a computer system is in use while it is being backed up, the possibility of files being open for reading or writing is real.
These files are organized into filesystems.
; Multiplexing : When there are many more computers to be backed up than there are destination storage devices, the ability to use a single storage device with several simultaneous backups can be useful.
Second, some backup programs can use checksums to avoid making redundant copies of files, and thus improve backup speed.
The most desirable RPO would be the point just prior to the data loss event.
Most data on modern computer systems is stored in discrete units, known as files.
A good example would be a tape library with restore times ranging from seconds to a few minutes.
Some implementations require integration with the source file system.
; Floppy disk : During the 1980s and early 1990s, many personal/home computer users associated backing up mostly with copying to floppy disks.
; Data security : In addition to preserving access to data for its owners, data must be restricted from unauthorized access.
A more sophisticated setup could include a computerized index, catalog, or relational database.
Disaster, data complexity, data value and increasing dependence upon ever-growing volumes of data all contribute to the anxiety around and dependence upon successful backups to ensure business continuity.
; Reverse delta : A reverse delta type repository stores a recent "mirror" of the source data and a series of differences between the mirror in its current state and its previous states.
; Near-line : Near-line storage is typically less accessible and less expensive than on-line storage, but still useful for backup data storage.
This is because the data being backed up changed in the period of time between when the backup started and when it finished.
The main disadvantages of hard disk backups are that they are easily damaged, especially while being transported (e.g., for off-site backups), and that their stability over periods of years is a relative unknown.
Generally it has safety properties similar to on-line storage.
Such impacts should be analyzed.
These offer several advantages.
; System metadata: Different operating systems have different ways of storing configuration information.
; Continuous data protection : Instead of scheduling periodic backups, the system immediately logs every change on the host system.
These media management methods are not mutually exclusive and are frequently combined to meet the user's needs.
Tape is a sequential access medium, so even though access times may be poor, the rate of continuously writing or reading data can actually be very fast.
The capacity offered from SSDs continues to grow and prices are gradually decreasing as they become more common.
On-line storage is quite vulnerable to being deleted or overwritten, either by accident, by intentional malevolent action, or in the wake of a data-deleting virus payload.
The process can also occur at the target storage device, sometimes referred to as inline or back-end deduplication.
For example, if a backup system uses a single tape each day to store the incremental backups for all the protected computers, restoring one of the computers could potentially require many tapes.
Refactoring could be used to consolidate all the backups for a single computer onto a single tape.
This eliminates the need to store duplicate copies of unchanged data: with full backups a lot of the data will be unchanged from what has been backed up previously.
Individuals and organizations with anything from one computer to thousands of computer systems all require protection of data.
This is the easiest to implement, but probably the least likely to achieve a high level of recoverability as it lacks automation.
Tape has typically had an order of magnitude better capacity-to-price ratio when compared to hard disk, but recently the ratios for tape and hard disk have become a lot closer.
There are, however, a number of drawbacks to remote backup services.
; Staging : Sometimes backup jobs are copied to a staging disk before being copied to tape.
:: Note: Vendors have standardized on the meaning of the terms "incremental backup" and "differential backup".
; Recovery point objective (RPO) : The point in time that the restarted infrastructure will reflect.
After that, a number of ''incremental'' backups are made after successive time periods.
First, Internet connections are usually slower than local data storage devices.
First, they allow data integrity to be verified without reference to the original file: if the file as stored on the backup medium has the same checksum as the saved value, then it is very probably correct.
Computer systems onto which the data can be restored and properly configured networks are necessary too.
This is making it more competitive with magnetic tape as a bulk storage medium.
Using an authentication mechanism is a good way to prevent the backup scheme from being used for unauthorized activity.
A reverse delta backup will start with a normal full backup.
close all files), take a snapshot, and then resume live operations.
It is important to recognize the limitations and human factors involved in any backup scheme.
; Differential : Each differential backup saves the data that has changed since the last full backup.
; Encryption : High capacity removable storage media such as backup tapes present a data security risk if they are lost or stolen.
This approach also reduces bandwidth required to send backup data to its target media.
The data repository only needs to store one copy of those files to be able to restore any one of those workstations.
Since a backup system contains at least one copy of all data considered worth saving, the data storage requirements can be significant.
By standard definition, a differential backup copies files that have been created or changed since the last full backup, regardless of whether any other differential backups have been made since then, whereas an incremental backup copies files that have been created or changed since the most recent backup of any type (full or incremental).
; Refactoring: The process of rearranging the backup sets in a data repository is known as refactoring.
One reason for this is that not all backup systems are able to reconstitute a computer system or other complex configuration such as a computer cluster, active directory server, or database server by simply restoring data from a backup.
; Magnetic tape : Magnetic tape has long been the most commonly used medium for bulk data storage, backup, archiving, and interchange.
Those who perform backups need to know how successful the backups are, regardless of scale.
Any backup scheme has some labor requirement, but complicated schemes have considerably higher labor requirements.
; Validation: Many backup programs use checksums or hashes to validate that the data was accurately copied.
In order to back up a file that is in use, it is vital that the entire backup represent a single-moment snapshot of the file, rather than a simple copy of a read-through.
The term fuzzy backup can be used to describe a backup of live data that looks like it ran correctly, but does not represent the state of the data at any single point in time.
This is particularly useful for the de-duplication process.
Backups have two distinct purposes.
; Full only / System imaging : A repository of this type contains complete system images taken at one or more specific points in time.
; Versioning file system : A versioning filesystem keeps track of all changes to a file and makes those changes accessible to the user.
A solid-state drive does not contain any movable parts unlike its magnetic drive counterpart, making it less susceptible to physical damage, and can have huge throughput in the order of 500Mbit/s to 6Gbit/s.
Because the data are not accessible via any computer except during limited periods in which they are written or read back, they are largely immune to a whole class of on-line backup failure modes.
The vault can be as simple as a system administrator's home office or as sophisticated as a disaster-hardened, temperature-controlled, high-security bunker with facilities for backup media storage.
This is especially useful for backup systems that do ''incrementals forever'' style backups.
This process is sometimes referred to as D2D2T, an acronym for Disk to Disk to Tape.
; Unstructured : An unstructured repository may simply be a stack of or CD-Rs or DVD-Rs with minimal information about what was backed up and when.
It differs from simple disk mirroring in that it enables a roll-back of the log and thus restoration of old images of data.
By backing up too much redundant data, the data repository will fill up too quickly.
For example, if 20 Windows workstations were backed up to the same data repository, they might share a common set of system files.
This tends to limit the use of such services to relatively small amounts of high value data.
; Incremental : An incremental style repository aims to make it more feasible to store backups from more points in time by organizing the data into increments of change between points in time.
An example of this is the Wayback versioning filesystem for Linux.
; Recovery time objective (RTO) : The amount of time elapsed between disaster and restoration of business functions.
; Solid state storage : Also known as flash memory, thumb drives, USB flash drives, CompactFlash, SmartMedia, Memory Stick, Secure Digital cards, etc., these devices are relatively expensive for their low capacity in comparison to hard disk drives, but are very convenient for backing up relatively low data volumes.
The backup data needs to be stored, and probably should be organized to a degree.
A good example is an internal hard disk or a disk array (maybe connected to SAN).
It has the advantage that only a maximum of two data sets are needed to restore the data.
Retaining backups after this period can lead to unwanted liability and sub-optimal use of storage media.
; Data retention period : Regulations and policy can lead to situations where backups are expected to be retained for a particular period, but not any further.
Organizing this storage space and managing the backup process can be a complicated undertaking.
Some simply check for openness and try again later.
Ultimately the backup service must itself use one of the above methods so this could be seen as a more complex way of doing traditional backups.
Either the database file must be locked to prevent changes, or a method must be implemented to ensure that the original snapshot is preserved long enough to be copied, all while changes are being preserved.
Every backup scheme should include dry runs that validate the reliability of the data being backed up.
But because this method also reads the free disk blocks that contain no useful data, this method can also be slower than conventional reading, especially when the filesystem is nearly empty.
An effective way to back up live data is to temporarily quiesce them (e.g.
For databases in particular, fuzzy backups are worthless.
Backing up an insufficient amount of data can eventually lead to the loss of critical information.
; Hot database backup: Some database management systems offer a means to generate a backup image of the database while it is online and usable ("hot").
This can be useful if there is a problem matching the speed of the final destination device with the source device as is frequently faced in network-based backup systems.
This is especially true for database files of all kinds.
Typically, a ''full'' backup (of all files) is made on one occasion (or at infrequent intervals) and serves as the reference point for an incremental backup set.
Thus many organizations rely on third-party or "independent" solutions to test, validate, and optimize their backup operations (backup reporting).
; Hard disk: The capacity-to-price ratio of hard disk has been rapidly improving for many years.
; Logging: In addition to the history of computer generated reports, activity and change logs are useful for monitoring backup system events.
Some new tape drives are even faster than modern hard disks.
Monitored backup requires software capable of pinging the monitoring center's servers in the case of errors.
Microsoft Windows keeps a registry of system information that is more difficult to restore than a typical file.
This technology is frequently used by computer technicians to record known good configurations.
Residential broadband is especially problematic as routine backups must use an upstream link that's usually much slower than the downstream link used only occasionally to retrieve a file from backup.
; Chain of trust : Removable storage media are physical items and must only be handled by trusted individuals.
; Costs of hardware, software, labor: All types of storage media have a finite capacity with a real cost.
The organisation could be as simple as a sheet of paper with a list of all backup media (CDs etc.)
Such a replica has fairly limited value as a backup, and should not be confused with an off-line backup.
As long as new data are being created and changes are being made, backups will need to be performed at frequent intervals.
; Open file backup: Many backup software packages feature the ability to handle open files in backup operations.
Backing up a file while it is being changed, in a manner that causes the first part of the backup to represent data ''before'' changes occur to be combined with later parts of the backup ''after'' the change results in a corrupted file that is unusable, as most large files contain internal references between their various parts that must remain consistent throughout the file.
The corresponding restore utility can selectively restore individual files or the entire volume at the operator's choice.
The main advantages of hard disk storage are low access times, availability, capacity and ease of use.
This is also known as a ''raw partition backup'' and is related to disk imaging.
Before data are sent to their storage locations, they are selected, extracted, and manipulated.
Part of the model is the backup rotation scheme.
If a backup extends past the defined backup window, a decision is made whether it is more beneficial to abort the backup or to lengthen the backup window.
The most relevant characteristic of an incremental backup is which reference point it uses to check for changes.
Any backup strategy starts with a concept of a data repository.
Upon a restore, the changes in the log files are reapplied to bring the copy of the database up-to-date (the point in time at which the initial hot backup ended).
: When attempting to understand the logistics of backing up open files, one must consider that the backup process could take several minutes to back up a large file such as a database.
; Authentication: Over the course of regular operations, the user accounts and/or system agents that perform the backups need to be authenticated at some level.
; Network bandwidth: Distributed backup systems can be affected by limited network bandwidth.
Access time will vary depending on whether the media are on-site or off-site.
A data repository model may be used to provide structure to the storage.
The power to copy all data off of or onto a system requires unrestricted access.
; Cold database backup: During a cold backup, the database is closed or locked and not available to users.
